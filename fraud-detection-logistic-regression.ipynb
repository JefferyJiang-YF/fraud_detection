{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1399887,"sourceType":"datasetVersion","datasetId":817870}],"dockerImageVersionId":30178,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Credit Card Fraud Case Study","metadata":{}},{"cell_type":"markdown","source":"### Importing basic modules","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.metrics import accuracy_score, mean_absolute_error ,mean_squared_error, confusion_matrix, median_absolute_error,classification_report, f1_score,recall_score,precision_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import HalvingRandomSearchCV,RandomizedSearchCV\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed = 39\n\ntrain = pd.read_csv('../input/fraud-detection/fraudTrain.csv')\ntest = pd.read_csv('../input/fraud-detection/fraudTest.csv')\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:12:00.409190Z","iopub.execute_input":"2024-04-10T07:12:00.409482Z","iopub.status.idle":"2024-04-10T07:12:10.775209Z","shell.execute_reply.started":"2024-04-10T07:12:00.409408Z","shell.execute_reply":"2024-04-10T07:12:10.774463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing Data","metadata":{}},{"cell_type":"code","source":"print(test.shape),print(train.shape)\nprint(test.isnull().sum())\nprint(train.isnull().sum())\nprint(test.info(), train.info())","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:12:10.776320Z","iopub.execute_input":"2024-04-10T07:12:10.776545Z","iopub.status.idle":"2024-04-10T07:12:15.151742Z","shell.execute_reply.started":"2024-04-10T07:12:10.776517Z","shell.execute_reply":"2024-04-10T07:12:15.150969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Cleaning ","metadata":{}},{"cell_type":"markdown","source":"Converting dob,trans_date_trans_time  column in both test & train to datetime data type and creating new 'trans_date' column - ","metadata":{}},{"cell_type":"code","source":"train['trans_date_trans_time']=pd.to_datetime(train['trans_date_trans_time'])\ntrain['trans_date']=train['trans_date_trans_time'].dt.strftime('%Y-%m-%d')\ntrain['trans_date']=pd.to_datetime(train['trans_date'])\ntrain['dob']=pd.to_datetime(train['dob'])\n\ntest['trans_date_trans_time']=pd.to_datetime(test['trans_date_trans_time'])\ntest['trans_date']=test['trans_date_trans_time'].dt.strftime('%Y-%m-%d')\ntest['trans_date']=pd.to_datetime(test['trans_date'])\ntest['dob']=pd.to_datetime(test['dob'])\ntest.trans_date.head(),test.dob.head(),train.trans_date.head(),train.dob.head()\n\ntrain.drop(\"Unnamed: 0\",axis=1,inplace=True)\ntest.drop(\"Unnamed: 0\",axis=1,inplace=True)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:12:15.152892Z","iopub.execute_input":"2024-04-10T07:12:15.153122Z","iopub.status.idle":"2024-04-10T07:12:31.158435Z","shell.execute_reply.started":"2024-04-10T07:12:15.153093Z","shell.execute_reply":"2024-04-10T07:12:31.157584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing unnamed column","metadata":{}},{"cell_type":"markdown","source":"### EDA, Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Categorical Variable Analysis","metadata":{}},{"cell_type":"code","source":"total = pd.concat([test,train])\nprint(total.info())\n\ntotal[\"is_fraud_cat\"]=total.is_fraud.apply(lambda x: \"T\" if x==1 else \"F\")\ntotal[\"is_fraud_cat\"].astype(\"object\")\n\ntotalcat=total.select_dtypes(include=['object'])\n\ntotal[totalcat.columns]","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:12:31.160241Z","iopub.execute_input":"2024-04-10T07:12:31.160476Z","iopub.status.idle":"2024-04-10T07:12:33.782036Z","shell.execute_reply.started":"2024-04-10T07:12:31.160446Z","shell.execute_reply":"2024-04-10T07:12:33.781154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'total' is your DataFrame and it contains 'is_fraud_cat' and 'category' columns\n# Create a count plot of the 'category' column for rows where 'is_fraud_cat' is \"T\"\nsns.countplot(total[total['is_fraud_cat'] == \"T\"].category)\n\n# Rotate the x-axis labels for better readability\nplt.xticks(rotation=90)\n\n# Save the figure\nplt.savefig('fraud_by_category.pdf', bbox_inches='tight', dpi=300)\n\n# Display the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:12:55.696374Z","iopub.execute_input":"2024-04-10T07:12:55.697060Z","iopub.status.idle":"2024-04-10T07:12:56.706783Z","shell.execute_reply.started":"2024-04-10T07:12:55.697026Z","shell.execute_reply":"2024-04-10T07:12:56.705947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots(figsize=(80,60))\n# plt.rcParams.update({'font.size': 60})\n# sns.countplot(total[total['is_fraud_cat']==\"T\"].state)\n# plt.xticks(rotation=45)\n# for p, label in zip(ax.patches, total[\"state\"].value_counts(sort=True,ascending=False).head(10)):\n#     ax.annotate(label, (p.get_x(), p.get_height()+0.25))\n# plt.title(\"Number of Credit Card Frauds by State\")\n\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:20:09.927091Z","iopub.execute_input":"2024-04-10T07:20:09.927882Z","iopub.status.idle":"2024-04-10T07:20:12.550260Z","shell.execute_reply.started":"2024-04-10T07:20:09.927843Z","shell.execute_reply":"2024-04-10T07:20:12.549489Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming 'total' is your DataFrame and it contains 'is_fraud_cat' and 'state' columns\n# Filter the data to include only fraudulent transactions\nfraud_data = total[total['is_fraud_cat'] == \"T\"]\n\n# Calculate the count of frauds by state and get the top 10 states\ntop_states = fraud_data['state'].value_counts().head(10).index\n\n# Filter the fraud_data to include only the top 10 states\ntop_fraud_data = fraud_data[fraud_data['state'].isin(top_states)]\n\n# Now create the count plot for only the top 10 states\nfig, ax = plt.subplots(figsize=(80, 60))\nplt.rcParams.update({'font.size': 60})\n\n# Use the filtered top_fraud_data for plotting\nsns.countplot(x='state', data=top_fraud_data, order=top_states)\n\nplt.xticks(rotation=45)\n\n# Annotate the bars with the count of frauds\nfor p in ax.patches:\n    ax.annotate(format(p.get_height(), '.1f'), \n                (p.get_x() + p.get_width() / 2., p.get_height()), \n                ha = 'center', va = 'center', \n                xytext = (0, 9), \n                textcoords = 'offset points')\n\nplt.title(\"Number of Credit Card Frauds by State\")\nplt.savefig('fraud_by_state.pdf', bbox_inches='tight', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:20:18.835629Z","iopub.execute_input":"2024-04-10T07:20:18.835884Z","iopub.status.idle":"2024-04-10T07:20:21.221094Z","shell.execute_reply.started":"2024-04-10T07:20:18.835852Z","shell.execute_reply":"2024-04-10T07:20:21.220349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\ndef randomcolor():\n    r = random.random()\n    b = random.random()\n    g = random.random()\n    rgb = [r,g,b]\n    return rgb\nplt.rcParams.update({'font.size': 20})\ntotal[total['is_fraud_cat']==\"T\"][\"city\"].value_counts(sort=True,ascending=False).head(10).plot(kind=\"bar\",color=randomcolor())\nplt.title(\"Number of Credit Card Frauds by City\")\nplt.savefig('fraud_by_city.pdf', bbox_inches='tight', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:21:48.818369Z","iopub.execute_input":"2024-04-10T07:21:48.818665Z","iopub.status.idle":"2024-04-10T07:21:49.443092Z","shell.execute_reply.started":"2024-04-10T07:21:48.818630Z","shell.execute_reply":"2024-04-10T07:21:49.442366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total[total['is_fraud_cat']==\"T\"][\"job\"].value_counts(sort=True,ascending=False).head(10).plot(kind=\"bar\",color=randomcolor())\nplt.title(\"Number of Credit Card Frauds by Job\")\nplt.savefig('fraud_by_job.pdf', bbox_inches='tight', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:22:54.239986Z","iopub.execute_input":"2024-04-10T07:22:54.240840Z","iopub.status.idle":"2024-04-10T07:22:55.060527Z","shell.execute_reply.started":"2024-04-10T07:22:54.240798Z","shell.execute_reply":"2024-04-10T07:22:55.059833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical Variable Analysis\nchecking the spread & skewness of all numerical variables","metadata":{}},{"cell_type":"code","source":"del total['is_fraud_cat']\nfrom scipy.stats import norm, skew\n\n#finding numerical columns\ntestnum= test.select_dtypes(include=np.number)\ntest[testnum.columns]\n\ntotal.isnull().sum()\n\ntotal[testnum.columns].info()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:35:25.516076Z","iopub.execute_input":"2024-04-10T07:35:25.516476Z","iopub.status.idle":"2024-04-10T07:35:28.285373Z","shell.execute_reply.started":"2024-04-10T07:35:25.516439Z","shell.execute_reply":"2024-04-10T07:35:28.284547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams.update({'font.size': 10})\nskewness = str(skew(total['amt']))\nsns.distplot(total['amt'],fit = norm,color = randomcolor())\nplt.title(\"Skewness of amt\"+\" = \"+skewness)\nplt.savefig('Skewness_of_amt.pdf', bbox_inches='tight', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:46:30.833957Z","iopub.execute_input":"2024-04-10T07:46:30.834816Z","iopub.status.idle":"2024-04-10T07:46:37.818269Z","shell.execute_reply.started":"2024-04-10T07:46:30.834771Z","shell.execute_reply":"2024-04-10T07:46:37.817553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skewness = str(skew(total['city_pop']))\nsns.distplot(total['city_pop'],fit = norm,color = randomcolor())\nplt.title(\"Skewness of population\"+\" = \"+skewness)\nplt.savefig('Skewness_of_pop.pdf', bbox_inches='tight', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:47:04.066114Z","iopub.execute_input":"2024-04-10T07:47:04.066431Z","iopub.status.idle":"2024-04-10T07:47:11.138628Z","shell.execute_reply.started":"2024-04-10T07:47:04.066397Z","shell.execute_reply":"2024-04-10T07:47:11.137751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skewness = str(skew(total['city_pop']))\nsns.distplot(total['cc_num'],fit = norm,color = randomcolor())\nplt.title(\"Skewness of cc_num\"+\" = \"+skewness)\nplt.savefig('Skewness_of_cc_num.pdf', bbox_inches='tight', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:48:47.428195Z","iopub.execute_input":"2024-04-10T07:48:47.428994Z","iopub.status.idle":"2024-04-10T07:48:55.111345Z","shell.execute_reply.started":"2024-04-10T07:48:47.428958Z","shell.execute_reply":"2024-04-10T07:48:55.110570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(total['is_fraud'],fit = norm,color = randomcolor())\nplt.title(\"Distribution of is_fraud\")\nplt.savefig('dis_is_fraud.pdf', bbox_inches='tight', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:51:04.376359Z","iopub.execute_input":"2024-04-10T07:51:04.377155Z","iopub.status.idle":"2024-04-10T07:51:11.411986Z","shell.execute_reply.started":"2024-04-10T07:51:04.377108Z","shell.execute_reply":"2024-04-10T07:51:11.411221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total.drop(['cc_num','merchant','first','last','street','zip','trans_num','unix_time'],axis=1,inplace=True)\n# total.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nax = sns.countplot(x=\"is_fraud\", data=total,color=randomcolor())\nfor p in ax.patches:\n     ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n        \nplt.savefig('bar_label.pdf', bbox_inches='tight', dpi=300)        \nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:52:15.959807Z","iopub.execute_input":"2024-04-10T07:52:15.960555Z","iopub.status.idle":"2024-04-10T07:52:16.455625Z","shell.execute_reply.started":"2024-04-10T07:52:15.960520Z","shell.execute_reply":"2024-04-10T07:52:16.454903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total[\"age\"] = total[\"trans_date\"]-total[\"dob\"]\ntotal[\"age\"]=total[\"age\"].astype('timedelta64[Y]')\nprint(total[\"age\"].head())\n# print(total.info())","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:55:04.216899Z","iopub.execute_input":"2024-04-10T07:55:04.217587Z","iopub.status.idle":"2024-04-10T07:55:04.349756Z","shell.execute_reply.started":"2024-04-10T07:55:04.217551Z","shell.execute_reply":"2024-04-10T07:55:04.348921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fraud=total[total[\"is_fraud\"]==1]\nfig, ax = plt.subplots()\nax.hist(fraud.age, edgecolor = \"black\", bins = 5, color=randomcolor())\nplt.title(\"Number of Credit Card Frauds by Age Groups\")\nplt.savefig('Fraud_by_age_group.pdf', bbox_inches='tight', dpi=300)  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:56:16.739998Z","iopub.execute_input":"2024-04-10T07:56:16.740866Z","iopub.status.idle":"2024-04-10T07:56:17.235568Z","shell.execute_reply.started":"2024-04-10T07:56:16.740823Z","shell.execute_reply":"2024-04-10T07:56:17.234845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total['trans_month'] = pd.DatetimeIndex(total['trans_date']).month\ntotal['trans_year'] = pd.DatetimeIndex(total['trans_date']).year\n\nimport calendar\ntotal['Month_name'] = total['trans_month'].apply(lambda x: calendar.month_abbr[x])","metadata":{"execution":{"iopub.status.busy":"2024-04-10T07:57:20.614887Z","iopub.execute_input":"2024-04-10T07:57:20.615301Z","iopub.status.idle":"2024-04-10T07:57:28.490359Z","shell.execute_reply.started":"2024-04-10T07:57:20.615255Z","shell.execute_reply":"2024-04-10T07:57:28.489577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(total[total[\"is_fraud\"]==1][\"Month_name\"],color=randomcolor())\nplt.title(\"Number of Credit Card Frauds by month\")\nplt.savefig('Fraud_by_month.pdf', bbox_inches='tight', dpi=300) \nplt.show()\ndel total['Month_name']","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:09:06.573341Z","iopub.execute_input":"2024-04-10T08:09:06.573694Z","iopub.status.idle":"2024-04-10T08:09:07.835850Z","shell.execute_reply.started":"2024-04-10T08:09:06.573657Z","shell.execute_reply":"2024-04-10T08:09:07.835081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(total[total[\"is_fraud\"]==1][\"gender\"],color=randomcolor())\nplt.title(\"Number of Credit Card Frauds by Gender\")\nplt.savefig('Fraud_by_gender.pdf', bbox_inches='tight', dpi=300) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:09:46.509921Z","iopub.execute_input":"2024-04-10T08:09:46.510660Z","iopub.status.idle":"2024-04-10T08:09:46.858273Z","shell.execute_reply.started":"2024-04-10T08:09:46.510599Z","shell.execute_reply":"2024-04-10T08:09:46.857419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(total[total[\"is_fraud\"]==1][\"trans_year\"],color=randomcolor())\nplt.title(\"Number of Credit Card Frauds by year\")\nplt.savefig('Fraud_by_year.pdf', bbox_inches='tight', dpi=300) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:16:13.564317Z","iopub.execute_input":"2024-04-10T08:16:13.564650Z","iopub.status.idle":"2024-04-10T08:16:13.782352Z","shell.execute_reply.started":"2024-04-10T08:16:13.564613Z","shell.execute_reply":"2024-04-10T08:16:13.781533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total['latitudinal_distance'] = abs(round(total['merch_lat']-total['lat'],3))\ntotal['longitudinal_distance'] = abs(round(total['merch_long']-total['long'],3))","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:16:23.149020Z","iopub.execute_input":"2024-04-10T08:16:23.149806Z","iopub.status.idle":"2024-04-10T08:16:23.184097Z","shell.execute_reply.started":"2024-04-10T08:16:23.149770Z","shell.execute_reply":"2024-04-10T08:16:23.183310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fraud = total[total[\"is_fraud\"]==1]\nfig, ax = plt.subplots()\nax.hist(fraud.latitudinal_distance, edgecolor = \"black\", bins = 5, color=randomcolor())\nplt.title(\"Number of Credit Card Frauds by latitudinal distance\")\nplt.savefig('Fraud_by_latitude.pdf', bbox_inches='tight', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:16:52.986727Z","iopub.execute_input":"2024-04-10T08:16:52.987411Z","iopub.status.idle":"2024-04-10T08:16:53.284253Z","shell.execute_reply.started":"2024-04-10T08:16:52.987368Z","shell.execute_reply":"2024-04-10T08:16:53.283519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.hist(fraud.longitudinal_distance, edgecolor = \"black\", bins = 5, color=randomcolor())\nplt.title(\"Number of Credit Card Frauds by longitudinal distance\")\nplt.savefig('Fraud_by_longtitude.pdf', bbox_inches='tight', dpi=300)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:17:10.021951Z","iopub.execute_input":"2024-04-10T08:17:10.022716Z","iopub.status.idle":"2024-04-10T08:17:10.463206Z","shell.execute_reply.started":"2024-04-10T08:17:10.022673Z","shell.execute_reply":"2024-04-10T08:17:10.462374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(total.info())\n\n# print(total.gender.value_counts())\n\ntotal.gender=total.gender.apply(lambda x: 1 if x==\"M\" else 0)\ntotal.gender.value_counts()\n\ndrop_cols = ['trans_date_trans_time','city','lat','long','job','dob','merch_lat','merch_long','trans_date','state']\ntotal=total.drop(drop_cols,axis=1)\n# total.info()\n\ntotal = pd.get_dummies(total,columns=['category'],drop_first=True)\nprint(total.info())\ntotal.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:17:21.278713Z","iopub.execute_input":"2024-04-10T08:17:21.279502Z","iopub.status.idle":"2024-04-10T08:17:23.938615Z","shell.execute_reply.started":"2024-04-10T08:17:21.279460Z","shell.execute_reply":"2024-04-10T08:17:23.937854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping final set of variables not useful for model building","metadata":{}},{"cell_type":"markdown","source":"### Model Building","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\n#create two different dataframe of majority and minority class \ndf_majority = total[(total['is_fraud']==0)] \ndf_minority = total[(total['is_fraud']==1)] \n\ndf_majority.shape,df_minority.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method 0: No sampling (BASELINE)","metadata":{}},{"cell_type":"code","source":"x_train_ori_col = list(total.columns)\nx_train_ori_col.remove('is_fraud')\nx_train_ori_col\n\nX_ori = total[x_train_ori_col]\nY_ori = total['is_fraud']\nprint(X_ori.info())\n\nX_train_ori, X_test_ori, Y_train_ori, Y_test_ori = train_test_split(\n X_ori, Y_ori, test_size=0.3, random_state=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc= StandardScaler()\nX_train_ori_std=sc.fit_transform(X_train_ori)\nX_test_ori_std = sc.fit_transform(X_test_ori)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FIND important feature based on original data distribution","metadata":{}},{"cell_type":"code","source":"logit_model_no_sampling = LogisticRegression(solver='liblinear',random_state=seed)\nlogit_model_no_sampling.fit(X_train_ori_std, Y_train_ori)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature= pd.DataFrame()\nfeature['column']= X_train_ori.columns\nfeature['importance']= logit_model_no_sampling.coef_[0]\nfeature.sort_values('importance', ascending=False, inplace=True)\nfeature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Finetuning","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nax = sns.countplot(x=\"is_fraud\", data=total,color=randomcolor())\nfor p in ax.patches:\n     ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`X_train_ori, X_test_ori, Y_train_ori, Y_test_ori`","metadata":{}},{"cell_type":"code","source":"Logit1_ori=LogisticRegression(solver='liblinear',random_state=seed)\n\nLogit1_ori.fit(X_train_ori_std,Y_train_ori)\n\nprint(\"Score of the model with X-train and Y-train is : \", str(round(Logit1_ori.score(X_train_ori,Y_train_ori)*100,2)),\"%\")\nprint(\"Score of the model with X-test and Y-test is : \", str(round(Logit1_ori.score(X_test_ori,Y_test_ori)*100,2)),\"%\")\n\nY_pred_ori=Logit1_ori.predict(X_test_ori_std)\n\nprint( \" Mean absolute error is \",( mean_absolute_error(Y_test_ori,Y_pred_ori)))\nprint(\" Mean squared  error is \" , mean_squared_error(Y_test_ori,Y_pred_ori))\nprint(\" Median absolute error is \" ,median_absolute_error(Y_test_ori,Y_pred_ori)) \nprint(\"Accuracy is \" , round(accuracy_score(Y_test_ori,Y_pred_ori)*100,2),\"%\")\nprint(\"F1 score: \", round(f1_score(Y_test_ori, Y_pred_ori, average='weighted')*100,2),\"%\")\nprint(\"Recall:\", round(recall_score(Y_test_ori, Y_pred_ori, average='weighted') * 100, 2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_ori_new = X_train_ori[[x for x in feature[feature[\"importance\"]>0].column]]\nX_test_ori_new = X_test_ori[[x for x in feature[feature[\"importance\"]>0].column]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_ori_sm = sm.add_constant(X_train_ori_new)\nlogm = sm.GLM(Y_train_ori, X_train_ori_sm, family = sm.families.Binomial())\nres = logm.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train_ori_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_ori_new.values, i) for i in range(X_train_ori_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since all columns have VIF < 5 , we'll continue with all columns\n#x_train_vif_adj = X_train_new[[x for x in list(vif[vif['VIF']<=5]['Features'])]]\nx_train_ori_vif_adj = X_train_ori_new\n#x_test_vif_adj = X_test_new[[x for x in list(vif[vif['VIF']<=5]['Features'])]]\nx_test_ori_vif_adj = X_test_ori_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc= StandardScaler()\nX_train_ori_vif_adj_std=sc.fit_transform(x_train_ori_vif_adj)\nX_test_ori_vif_adj_std = sc.fit_transform(x_test_ori_vif_adj)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Logit2_ori=LogisticRegression(solver='liblinear',random_state=seed)\n\nLogit2_ori.fit(X_train_ori_vif_adj_std,Y_train_ori)\n\nprint(\"Score of the model with X-train and Y-train is : \", str(round(Logit2_ori.score(X_train_ori_vif_adj_std,Y_train_ori)*100,2)),\"%\")\nprint(\"Score of the model with X-test and Y-test is : \", str(round(Logit2_ori.score(X_test_ori_vif_adj_std,Y_test_ori)*100,2)),\"%\")\n\nY_pred_ori=Logit2_ori.predict(X_test_ori_vif_adj_std)\n\nprint( \" Mean absolute error is \",( mean_absolute_error(Y_test_ori,Y_pred_ori)))\nprint(\" Mean squared  error is \" , mean_squared_error(Y_test_ori,Y_pred_ori))\nprint(\" Median absolute error is \" ,median_absolute_error(Y_test_ori,Y_pred_ori)) \nprint(\"Accuracy is \" , round(accuracy_score(Y_test_ori,Y_pred_ori)*100,2),\"%\")\nprint(\"F1 score: \", round(f1_score(Y_test_ori, Y_pred_ori, average='weighted')*100,2),\"%\")\nprint(\"Recall:\", round(recall_score(Y_test_ori, Y_pred_ori, average='weighted') * 100, 2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = confusion_matrix(Y_test_ori,Y_pred_ori, labels=[1,0])\nprint('Confusion matrix : \\n',matrix)\n\n\ntp, fn, fp, tn = confusion_matrix(Y_test_ori,Y_pred_ori,labels=[1,0]).reshape(-1)\nprint('Outcome values : \\n', tp, fn, fp, tn)\n\n\nmatrix = classification_report(Y_test_ori,Y_pred_ori,labels=[1,0])\nprint('Classification report : \\n',matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m0_lg_Recall = recall_score(Y_test_ori, Y_pred_ori, average='macro')\nm0_lg_Precision = precision_score(Y_test_ori, Y_pred_ori,average='macro')\nm0_lg_f1 = f1_score(Y_test_ori, Y_pred_ori,average='macro')\nm0_lg_accuracy = accuracy_score(Y_test_ori, Y_pred_ori)\nm0_lg_mae = mean_absolute_error(Y_test_ori,Y_pred_ori)\nm0_lg_mse = mean_squared_error(Y_test_ori,Y_pred_ori)\n\nm0_ndf = [(m0_lg_Recall, m0_lg_Precision, m0_lg_f1, m0_lg_accuracy,m0_lg_mae,m0_lg_mse)]\nm0_lg_score = pd.DataFrame(data = m0_ndf, columns=['Recall','Precision','F1 Score', 'Accuracy','MAE','MSE'])\nm0_lg_score.insert(0, 'Logistic Regression with', 'No Oversampling')\nm0_lg_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\n# 假设你已经有了预测结果和真实标签，计算ROC曲线的坐标\nfpr, tpr, thresholds = roc_curve(Y_test_ori, Logit2_ori.predict_proba(X_test_ori_vif_adj_std)[:,1])\nroc_auc = auc(fpr, tpr)\n\n# 绘制ROC曲线\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n\n# 添加对角线\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\n# 设定坐标轴标签和图的标题\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\n\n# 将图例放置在图表外的右侧\nplt.legend(loc=\"lower right\", bbox_to_anchor=(1.05, 0.5), borderaxespad=0.)\n\n# 调整图表边界，为图例腾出空间\nplt.subplots_adjust(right=0.75)\n\n# 展示图表\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method 1: Correcting the imbalance discovered by using resample","metadata":{}},{"cell_type":"code","source":"# upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,    # sample with replacement\n                                 n_samples= 1842743, # to match majority class\n                                 random_state=seed)  # reproducible results\ndf_minority_upsampled.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine majority class with upsampled minority class\ntotal_upsampled = pd.concat([df_minority_upsampled, df_majority])\ntotal_upsampled.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_cols = list(total_upsampled.columns)\nx_cols.remove('is_fraud')\n# x_cols\n\nX = total_upsampled[x_cols]\nY = total_upsampled['is_fraud']\n# X.info()\n\nX_train, X_test, Y_train, Y_test = train_test_split(\n X, Y, test_size=0.3, random_state=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling the x variables","metadata":{}},{"cell_type":"code","source":"sc= StandardScaler()\nX_train_std=sc.fit_transform(X_train)\nX_test_std = sc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importances","metadata":{}},{"cell_type":"code","source":"logit_model= LogisticRegression(solver='liblinear',random_state=seed)\nlogit_model.fit(X_train_std, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# 假设Y_train已经通过train_test_split获取\nunique, counts = np.unique(Y_train, return_counts=True)\nclass_counts = dict(zip(unique, counts))\n\nprint(\"类别数量:\", class_counts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature= pd.DataFrame()\nfeature['column']= X_train.columns\nfeature['importance']= logit_model.coef_[0]\nfeature.sort_values('importance', ascending=False, inplace=True)\nfeature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nax = sns.countplot(x=\"is_fraud\", data=total_upsampled,color=randomcolor())\nfor p in ax.patches:\n     ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Logit1=LogisticRegression(solver='liblinear',random_state=seed)\n\nLogit1.fit(X_train_std,Y_train)\n\nprint(\"Score of the model with X-train and Y-train is : \", str(round(Logit1.score(X_train,Y_train)*100,2)),\"%\")\nprint(\"Score of the model with X-test and Y-test is : \", str(round(Logit1.score(X_test,Y_test)*100,2)),\"%\")\n\nY_pred=Logit1.predict(X_test_std)\n\nprint( \" Mean absolute error is \",( mean_absolute_error(Y_test,Y_pred)))\nprint(\" Mean squared  error is \" , mean_squared_error(Y_test,Y_pred))\nprint(\" Median absolute error is \" ,median_absolute_error(Y_test,Y_pred)) \nprint(\"Accuracy is \" , round(accuracy_score(Y_test,Y_pred)*100,2),\"%\")\nprint(\"F1 score: \", round(f1_score(Y_test, Y_pred, average='weighted')*100,2),\"%\")\nprint(\"Recall:\", round(recall_score(Y_test, Y_pred, average='weighted') * 100, 2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method 1: Fine Tuning","metadata":{}},{"cell_type":"code","source":"X_train_new = X_train[[x for x in feature[feature[\"importance\"]>0].column]]\nX_test_new = X_test[[x for x in feature[feature[\"importance\"]>0].column]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"checking p values & variance inflation factor","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train_new)\nlogm = sm.GLM(Y_train, X_train_sm, family = sm.families.Binomial())\nres = logm.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"since all columns have VIF < 5 , we'll continue with all columns","metadata":{}},{"cell_type":"code","source":"#x_train_vif_adj = X_train_new[[x for x in list(vif[vif['VIF']<=5]['Features'])]]\nx_train_vif_adj = X_train_new\n#x_test_vif_adj = X_test_new[[x for x in list(vif[vif['VIF']<=5]['Features'])]]\nx_test_vif_adj = X_test_new\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scaling the new test and train sets","metadata":{}},{"cell_type":"code","source":"sc= StandardScaler()\nX_train_vif_adj_std=sc.fit_transform(x_train_vif_adj)\nX_test_vif_adj_std = sc.fit_transform(x_test_vif_adj)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training a new Logistic Regression Model to reflect the changes-","metadata":{}},{"cell_type":"code","source":"Logit2=LogisticRegression(solver='liblinear',random_state=seed)\n\nLogit2.fit(X_train_vif_adj_std,Y_train)\n\nprint(\"Score of the model with X-train and Y-train is : \", str(round(Logit2.score(X_train_vif_adj_std,Y_train)*100,2)),\"%\")\nprint(\"Score of the model with X-test and Y-test is : \", str(round(Logit2.score(X_test_vif_adj_std,Y_test)*100,2)),\"%\")\n\nY_pred=Logit2.predict(X_test_vif_adj_std)\n\nprint( \" Mean absolute error is \",( mean_absolute_error(Y_test,Y_pred)))\nprint(\" Mean squared  error is \" , mean_squared_error(Y_test,Y_pred))\nprint(\" Median absolute error is \" ,median_absolute_error(Y_test,Y_pred)) \nprint(\"Accuracy is \" , round(accuracy_score(Y_test,Y_pred)*100,2),\"%\")\nprint(\"F1 score: \", round(f1_score(Y_test, Y_pred, average='weighted')*100,2),\"%\")\nprint(\"Recall:\", round(recall_score(Y_test, Y_pred, average='weighted') * 100, 2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = confusion_matrix(Y_test,Y_pred, labels=[1,0])\nprint('Confusion matrix : \\n',matrix)\n\n\ntp, fn, fp, tn = confusion_matrix(Y_test,Y_pred,labels=[1,0]).reshape(-1)\nprint('Outcome values : \\n', tp, fn, fp, tn)\n\n\nmatrix = classification_report(Y_test,Y_pred,labels=[1,0])\nprint('Classification report : \\n',matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1_lg_Recall = recall_score(Y_test, Y_pred, average='macro')\nm1_lg_Precision = precision_score(Y_test, Y_pred, average='macro')\nm1_lg_f1 = f1_score(Y_test, Y_pred, average='macro')\nm1_lg_accuracy = accuracy_score(Y_test, Y_pred)\nm1_lg_mae = mean_absolute_error(Y_test,Y_pred)\nm1_lg_mse = mean_squared_error(Y_test,Y_pred)\n\nm1_ndf = [(m1_lg_Recall, m1_lg_Precision, m1_lg_f1, m1_lg_accuracy,m1_lg_mae,m1_lg_mse)]\nm1_lg_score = pd.DataFrame(data = m1_ndf, columns=['Recall','Precision','F1 Score', 'Accuracy','MAE','MSE'])\nm1_lg_score.insert(1, 'Logistic Regression with', 'Resampling')\nm1_lg_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\n# 假设你已经有了预测结果和真实标签，计算ROC曲线的坐标\nfpr, tpr, thresholds = roc_curve(Y_test, Logit2.predict_proba(X_test_vif_adj_std)[:,1])\nroc_auc = auc(fpr, tpr)\n\n# 绘制ROC曲线\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n\n# 添加对角线\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\n# 设定坐标轴标签和图的标题\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\n\n# 将图例放置在图表外的右侧\nplt.legend(loc=\"lower right\", bbox_to_anchor=(1.05, 0.5), borderaxespad=0.)\n\n# 调整图表边界，为图例腾出空间\nplt.subplots_adjust(right=0.75)\n\n# 展示图表\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_roc_curve(Logit2, X_test_vif_adj_std, Y_test)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method 2: Random Resampling Imbalanced Datasets","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n# define oversampling strategy\nros = RandomOverSampler(random_state=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_ori_col = list(total.columns)\nx_train_ori_col.remove('is_fraud')\nx_train_ori_col\n\nX_ori = total[x_train_ori_col]\nY_ori = total['is_fraud']\n# print(X_ori.info())\n\nprint('BEFORE...')\nprint('Genuine:', Y_ori.value_counts()[0], '/', round(Y_ori.value_counts()[0]/len(Y_ori) * 100,2), '% of the dataset')\nprint('Frauds:', Y_ori.value_counts()[1], '/',round(Y_ori.value_counts()[1]/len(Y_ori) * 100,2), '% of the dataset')\n\nprint('AFTER...')\nX_m2_over, Y_m2_over = ros.fit_resample(X_ori, Y_ori)\nprint('Genuine:', Y_m2_over.value_counts()[0], '/', round(Y_m2_over.value_counts()[0]/len(Y_m2_over) * 100,2), '% of the dataset')\nprint('Frauds:', Y_m2_over.value_counts()[1], '/',round(Y_m2_over.value_counts()[1]/len(Y_m2_over) * 100,2), '% of the dataset')\n\nX_train, X_test, Y_train, Y_test = train_test_split(\n X_m2_over, Y_m2_over, test_size=0.3, random_state=seed)\nsc= StandardScaler()\n\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance Exploration","metadata":{}},{"cell_type":"code","source":"logit_m2_model= LogisticRegression(solver='liblinear',random_state=seed)\nlogit_m2_model.fit(X_train_std, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_m2= pd.DataFrame()\nfeature_m2['column']= X_train_ori.columns\nfeature_m2['importance']= logit_m2_model.coef_[0]\nfeature_m2.sort_values('importance', ascending=False, inplace=True)\nfeature_m2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"Logit1_m2=LogisticRegression(solver='liblinear',random_state=seed)\n\nLogit1_m2.fit(X_train_std,Y_train)\n\nprint(\"Score of the model with X-train and Y-train is : \", str(round(Logit1_m2.score(X_train,Y_train)*100,2)),\"%\")\nprint(\"Score of the model with X-test and Y-test is : \", str(round(Logit1_m2.score(X_test,Y_test)*100,2)),\"%\")\n\nY_pred_m2=Logit1_m2.predict(X_test_std)\n\nprint( \" Mean absolute error is \",( mean_absolute_error(Y_test,Y_pred_m2)))\nprint(\" Mean squared  error is \" , mean_squared_error(Y_test,Y_pred_m2))\nprint(\" Median absolute error is \" ,median_absolute_error(Y_test,Y_pred_m2)) \nprint(\"Accuracy is \" , round(accuracy_score(Y_test,Y_pred_m2)*100,2),\"%\")\nprint(\"F1 score: \", round(f1_score(Y_test, Y_pred_m2, average='weighted')*100,2),\"%\")\nprint(\"Recall:\", round(recall_score(Y_test, Y_pred_m2, average='weighted') * 100, 2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_new=X_train[[x for x in feature_m2[feature_m2[\"importance\"]>0].column]]\nX_test_new=X_test[[x for x in feature_m2[feature_m2[\"importance\"]>0].column]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_m2_sm = sm.add_constant(X_train_new)\nlogm = sm.GLM(Y_train, X_train_m2_sm, family = sm.families.Binomial())\nres = logm.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since all columns have VIF < 5 , we'll continue with all columns\n#x_train_vif_adj = X_train_new[[x for x in list(vif[vif['VIF']<=5]['Features'])]]\nx_train_vif_adj = X_train_new\n#x_test_vif_adj = X_test_new[[x for x in list(vif[vif['VIF']<=5]['Features'])]]\nx_test_vif_adj = X_test_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc= StandardScaler()\nX_train_vif_adj_std=sc.fit_transform(x_train_vif_adj)\nX_test_vif_adj_std = sc.fit_transform(x_test_vif_adj)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Logit2_m2=LogisticRegression(solver='liblinear',random_state=seed)\n\nLogit2_m2.fit(X_train_vif_adj_std,Y_train)\n\nprint(\"Score of the model with X-train and Y-train is : \", str(round(Logit2_m2.score(X_train_vif_adj_std,Y_train)*100,2)),\"%\")\nprint(\"Score of the model with X-test and Y-test is : \", str(round(Logit2_m2.score(X_train_vif_adj_std,Y_train)*100,2)),\"%\")\n\nY_pred=Logit2_m2.predict(X_test_vif_adj_std)\n\nprint( \" Mean absolute error is \",( mean_absolute_error(Y_test,Y_pred)))\nprint(\" Mean squared  error is \" , mean_squared_error(Y_test,Y_pred))\nprint(\" Median absolute error is \" ,median_absolute_error(Y_test,Y_pred)) \nprint(\"Accuracy is \" , round(accuracy_score(Y_test,Y_pred)*100,2),\"%\")\nprint(\"F1 score: \", round(f1_score(Y_test, Y_pred, average='weighted')*100,2),\"%\")\nprint(\"Recall:\", round(recall_score(Y_test, Y_pred, average='weighted') * 100, 2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = confusion_matrix(Y_test,Y_pred, labels=[1,0])\nprint('Confusion matrix : \\n',matrix)\n\n\ntp, fn, fp, tn = confusion_matrix(Y_test,Y_pred,labels=[1,0]).reshape(-1)\nprint('Outcome values : \\n', tp, fn, fp, tn)\n\n\nmatrix = classification_report(Y_test,Y_pred,labels=[1,0])\nprint('Classification report : \\n',matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m2_lg_Recall = recall_score(Y_test, Y_pred, average='macro')\nm2_lg_Precision = precision_score(Y_test, Y_pred, average='macro')\nm2_lg_f1 = f1_score(Y_test, Y_pred, average='macro')\nm2_lg_accuracy = accuracy_score(Y_test, Y_pred)\nm2_lg_mae = mean_absolute_error(Y_test,Y_pred)\nm2_lg_mse = mean_squared_error(Y_test,Y_pred)\n\nm2_ndf = [(m2_lg_Recall, m2_lg_Precision, m2_lg_f1, m2_lg_accuracy,m2_lg_mae,m2_lg_mse)]\nm2_lg_score = pd.DataFrame(data = m2_ndf, columns=['Recall','Precision','F1 Score', 'Accuracy','MAE','MSE'])\nm2_lg_score.insert(0, 'Logistic Regression with', 'Random resampling')\nm2_lg_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\n# 假设你已经有了预测结果和真实标签，计算ROC曲线的坐标\nfpr, tpr, thresholds = roc_curve(Y_test, Logit2_m2.predict_proba(X_test_vif_adj_std)[:,1])\nroc_auc = auc(fpr, tpr)\n\n# 绘制ROC曲线\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n\n# 添加对角线\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\n# 设定坐标轴标签和图的标题\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\n\n# 将图例放置在图表外的右侧\nplt.legend(loc=\"lower right\", bbox_to_anchor=(1.05, 0.5), borderaxespad=0.)\n\n# 调整图表边界，为图例腾出空间\nplt.subplots_adjust(right=0.75)\n\n# 展示图表\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_roc_curve(Logit2_m2, X_test_vif_adj_std, Y_test)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method 3: SMOTE (Synthetic Minority Oversampling Technique)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T16:46:03.172771Z","iopub.execute_input":"2024-04-08T16:46:03.173064Z","iopub.status.idle":"2024-04-08T16:46:03.176844Z","shell.execute_reply.started":"2024-04-08T16:46:03.173029Z","shell.execute_reply":"2024-04-08T16:46:03.176081Z"}}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_ori_col = list(total.columns)\nx_train_ori_col.remove('is_fraud')\nx_train_ori_col\n\nX_ori = total[x_train_ori_col]\nY_ori = total['is_fraud']\n# print(X_ori.info())\n\nprint('BEFORE...')\nprint('Genuine:', Y_ori.value_counts()[0], '/', round(Y_ori.value_counts()[0]/len(Y_ori) * 100,2), '% of the dataset')\nprint('Frauds:', Y_ori.value_counts()[1], '/',round(Y_ori.value_counts()[1]/len(Y_ori) * 100,2), '% of the dataset')\n\nprint('AFTER...')\nX_m3_over, Y_m3_over = smote.fit_resample(X_ori, Y_ori)\nprint('Genuine:', Y_m3_over.value_counts()[0], '/', round(Y_m3_over.value_counts()[0]/len(Y_m3_over) * 100,2), '% of the dataset')\nprint('Frauds:', Y_m3_over.value_counts()[1], '/',round(Y_m3_over.value_counts()[1]/len(Y_m3_over) * 100,2), '% of the dataset')\n\nX_train, X_test, Y_train, Y_test = train_test_split(\n X_m3_over, Y_m3_over, test_size=0.3, random_state=seed)\nsc= StandardScaler()\n\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance Exploration","metadata":{}},{"cell_type":"code","source":"logit_m3_model= LogisticRegression(solver='liblinear',random_state=seed)\nlogit_m3_model.fit(X_train_std, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_m3= pd.DataFrame()\nfeature_m3['column']= X_train_ori.columns\nfeature_m3['importance']= logit_m2_model.coef_[0]\nfeature_m3.sort_values('importance', ascending=False, inplace=True)\nfeature_m3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{}},{"cell_type":"code","source":"Logit1_m3=LogisticRegression(solver='liblinear',random_state=seed)\n\nLogit1_m3.fit(X_train_std,Y_train)\n\nprint(\"Score of the model with X-train and Y-train is : \", str(round(Logit1_m3.score(X_train,Y_train)*100,2)),\"%\")\nprint(\"Score of the model with X-test and Y-test is : \", str(round(Logit1_m3.score(X_test,Y_test)*100,2)),\"%\")\n\nY_pred=Logit1_m2.predict(X_test_std)\n\nprint( \" Mean absolute error is \",( mean_absolute_error(Y_test,Y_pred)))\nprint(\" Mean squared  error is \" , mean_squared_error(Y_test,Y_pred))\nprint(\" Median absolute error is \" ,median_absolute_error(Y_test,Y_pred)) \nprint(\"Accuracy is \" , round(accuracy_score(Y_test,Y_pred)*100,2),\"%\")\nprint(\"F1 score: \", round(f1_score(Y_test, Y_pred, average='weighted')*100,2),\"%\")\nprint(\"Recall:\", round(recall_score(Y_test, Y_pred, average='weighted') * 100, 2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_new=X_train[[x for x in feature_m3[feature_m3[\"importance\"]>0].column]]\nX_test_new=X_test[[x for x in feature_m3[feature_m3[\"importance\"]>0].column]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_m3_sm = sm.add_constant(X_train_new)\nlogm = sm.GLM(Y_train, X_train_m3_sm, family = sm.families.Binomial())\nres = logm.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since all columns have VIF < 5 , we'll continue with all columns\n#x_train_vif_adj = X_train_new[[x for x in list(vif[vif['VIF']<=5]['Features'])]]\nx_train_vif_adj = X_train_new\n#x_test_vif_adj = X_test_new[[x for x in list(vif[vif['VIF']<=5]['Features'])]]\nx_test_vif_adj = X_test_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc= StandardScaler()\nX_train_vif_adj_std=sc.fit_transform(x_train_vif_adj)\nX_test_vif_adj_std = sc.fit_transform(x_test_vif_adj)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Logit2_m3=LogisticRegression(solver='liblinear',random_state=seed)\n\nLogit2_m3.fit(X_train_vif_adj_std,Y_train)\n\nprint(\"Score of the model with X-train and Y-train is : \", str(round(Logit2_m3.score(X_train_vif_adj_std,Y_train)*100,2)),\"%\")\nprint(\"Score of the model with X-test and Y-test is : \", str(round(Logit2_m3.score(X_train_vif_adj_std,Y_train)*100,2)),\"%\")\n\nY_pred=Logit2_m3.predict(X_test_vif_adj_std)\n\nprint( \" Mean absolute error is \",( mean_absolute_error(Y_test,Y_pred)))\nprint(\" Mean squared  error is \" , mean_squared_error(Y_test,Y_pred))\nprint(\" Median absolute error is \" ,median_absolute_error(Y_test,Y_pred)) \nprint(\"Accuracy is \" , round(accuracy_score(Y_test,Y_pred)*100,2),\"%\")\nprint(\"F1 score: \", round(f1_score(Y_test, Y_pred, average='weighted')*100,2),\"%\")\nprint(\"Recall:\", round(recall_score(Y_test, Y_pred, average='weighted') * 100, 2), \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = confusion_matrix(Y_test,Y_pred, labels=[1,0])\nprint('Confusion matrix : \\n',matrix)\n\n\ntp, fn, fp, tn = confusion_matrix(Y_test,Y_pred,labels=[1,0]).reshape(-1)\nprint('Outcome values : \\n', tp, fn, fp, tn)\n\n\nmatrix = classification_report(Y_test,Y_pred,labels=[1,0])\nprint('Classification report : \\n',matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m3_lg_Recall = recall_score(Y_test, Y_pred, average='macro')\nm3_lg_Precision = precision_score(Y_test, Y_pred, average='macro')\nm3_lg_f1 = f1_score(Y_test, Y_pred, average='macro')\nm3_lg_accuracy = accuracy_score(Y_test, Y_pred)\nm3_lg_mae = mean_absolute_error(Y_test,Y_pred)\nm3_lg_mse = mean_squared_error(Y_test,Y_pred)\n\nm3_ndf = [(m3_lg_Recall, m3_lg_Precision, m3_lg_f1, m3_lg_accuracy,m3_lg_mae,m3_lg_mse)]\nm3_lg_score = pd.DataFrame(data = m3_ndf, columns=['Recall','Precision','F1 Score', 'Accuracy','MAE','MSE'])\nm3_lg_score.insert(0, 'Logistic Regression with', 'SMOTE')\nm3_lg_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\n# 假设你已经有了预测结果和真实标签，计算ROC曲线的坐标\nfpr, tpr, thresholds = roc_curve(Y_test, Logit2_m3.predict_proba(X_test_vif_adj_std)[:,1])\nroc_auc = auc(fpr, tpr)\n\n# 绘制ROC曲线\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n\n# 添加对角线\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\n# 设定坐标轴标签和图的标题\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\n\n# 将图例放置在图表外的右侧\nplt.legend(loc=\"lower right\", bbox_to_anchor=(1.05, 0.5), borderaxespad=0.)\n\n# 调整图表边界，为图例腾出空间\nplt.subplots_adjust(right=0.75)\n\n# 展示图表\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_result_report = pd.concat([m0_lg_score, m1_lg_score, m2_lg_score, m3_lg_score], ignore_index=True, sort=False)\nfinal_result_report.sort_values(by=['Recall'], ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_roc_curve(Logit2_m3, X_test_vif_adj_std, Y_test)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method 4: Undersampling using Tomek Links (Future) TOO SOLW","metadata":{}},{"cell_type":"code","source":"# from imblearn.under_sampling import TomekLinks\n\n# # define the undersampling method\n# tomekU = TomekLinks()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x_train_ori_col = list(total.columns)\n# x_train_ori_col.remove('is_fraud')\n# x_train_ori_col\n\n# X_ori = total[x_train_ori_col]\n# Y_ori = total['is_fraud']\n# # print(X_ori.info())\n\n# print('BEFORE...')\n# print('Genuine:', Y_ori.value_counts()[0], '/', round(Y_ori.value_counts()[0]/len(Y_ori) * 100,2), '% of the dataset')\n# print('Frauds:', Y_ori.value_counts()[1], '/',round(Y_ori.value_counts()[1]/len(Y_ori) * 100,2), '% of the dataset')\n\n# print('AFTER...')\n# X_underT, y_underT = tomekU.fit_resample(X_ori, Y_ori)\n# print('Genuine:', y_underT.value_counts()[0], '/', round(y_underT.value_counts()[0]/len(y_underT) * 100,2), '% of the dataset')\n# print('Frauds:', y_underT.value_counts()[1], '/',round(y_underT.value_counts()[1]/len(y_underT) * 100,2), '% of the dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method 5: Combining SMOTE and Tomek Links OR BorderlineSMOTE (Future) TOO SLOW!\n","metadata":{}},{"cell_type":"code","source":"# from imblearn.over_sampling import BorderlineSMOTE\n# import logging\n\n# # 配置日志记录器\n# logging.basicConfig(level=logging.INFO)\n\n# # 假设 X_ori 和 Y_ori 是您的原始特征和目标变量\n# Borderline_smote = BorderlineSMOTE(random_state=seed)\n\n# from imblearn.combine import SMOTETomek\n\n# # 假设 X_ori 和 Y_ori 表示您的原始特征和目标变量\n# smotetomek = SMOTETomek(random_state=seed)\n# X_resampled, Y_resampled = smotetomek.fit_resample(X_ori, Y_ori)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x_train_ori_col = list(total.columns)\n# x_train_ori_col.remove('is_fraud')\n# x_train_ori_col\n\n# X_ori = total[x_train_ori_col]\n# Y_ori = total['is_fraud']\n# # print(X_ori.info())\n\n# print('BEFORE...')\n# print('Genuine:', Y_ori.value_counts()[0], '/', round(Y_ori.value_counts()[0]/len(Y_ori) * 100,2), '% of the dataset')\n# print('Frauds:', Y_ori.value_counts()[1], '/',round(Y_ori.value_counts()[1]/len(Y_ori) * 100,2), '% of the dataset')\n\n# print('AFTER...')\n# X_m5_over, Y_m5_over = Borderline_smote.fit_resample(X_ori, Y_ori)\n# print('Genuine:', Y_m5_over.value_counts()[0], '/', round(Y_m5_over.value_counts()[0]/len(Y_m5_over) * 100,2), '% of the dataset')\n# print('Frauds:', Y_m5_over.value_counts()[1], '/',round(Y_m5_over.value_counts()[1]/len(Y_m5_over) * 100,2), '% of the dataset')\n\n# X_train, X_test, Y_train, Y_test = train_test_split(\n#  Y_m5_over, Y_m5_over, test_size=0.3, random_state=seed)\n# sc= StandardScaler()\n\n# X_train_std = sc.fit_transform(X_train)\n# X_test_std = sc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree","metadata":{}},{"cell_type":"code","source":"# dtc = DecisionTreeClassifier()\n# dtc.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importances using Decision Tree","metadata":{}},{"cell_type":"code","source":"# importance = dtc.feature_importances_\n# for i,v in enumerate(importance):\n#     print(X_train.columns[int(i)],\"- \",v)\n# plt.bar([x for x in range(len(importance))], importance)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Score the X-train with Y-train is : \", dtc.score(X_train,Y_train))\n# print(\"Score the X-test  with Y-test  is : \", dtc.score(X_test,Y_test))\n\n# Y_pred=dtc.predict(X_test)\n\n# print( \" Mean absolute error is \", mean_absolute_error(Y_test,Y_pred))\n# print(\" Mean squared  error is \" , mean_squared_error(Y_test,Y_pred))\n# print(\" Median absolute error is \" ,median_absolute_error(Y_test,Y_pred)) \n# print(\"Accuracy score \" , accuracy_score(Y_test,Y_pred))\n# print(\"F1 score: \", round(f1_score(Y_test, Y_pred, average='weighted')*100,2),\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perfect score on training set indicates overfitting. Using hyperparameter tuning.\n### Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# #Normal Randomised Search takes too much time to execute on a dataset this large.\n# \"\"\"dtc1 = DecisionTreeClassifier()\n\n\n# params_dtc = {  \n#     \"splitter\":[\"best\"],\n#     'max_depth': [10, 20, 50, 100, 200],\n#     'min_samples_leaf': [10, 20, 50, 100, 200],\n#     'min_samples_split' : [10, 20, 50, 100, 200],\n#     'criterion': [\"gini\", \"entropy\"]\n# }\n# random_search=RandomizedSearchCV(estimator=dtc1,param_distributions = params_dtc, scoring = 'f1',cv=5,n_iter=100)\n# random_search.fit(X_train,Y_train)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since dataset is very large(close to 1.8 million rows originally and even more after treating for unbalanced condition),\nwe will use halving randomized search cross validation which is an experimental variant of the randomised search, much  faster compared to either randomised search or grid search cross validation.\n","metadata":{}},{"cell_type":"code","source":"# dtc1 = DecisionTreeClassifier()\n\n# params_dtc = {\n#     'max_depth': [10, 20, 50, 100, 200],\n#     'min_samples_leaf': [10, 20, 50, 100, 200],\n#     'min_samples_split' : [10, 20, 50, 100, 200],\n#     'criterion': [\"gini\", \"entropy\"]\n# } \n\n# halving_random_search=HalvingRandomSearchCV(estimator=dtc1,param_distributions = params_dtc,cv=5)\n# halving_random_search.fit(X_train,Y_train)\n# print(halving_random_search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(halving_random_search.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dtc2 = DecisionTreeClassifier(min_samples_split= 100, min_samples_leaf= 20, max_depth= 200, criterion= 'gini')\n# dtc2.fit(X_train,Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Score the X-train with Y-train is : \", dtc2.score(X_train,Y_train))\n# print(\"Score the X-test  with Y-test  is : \", dtc2.score(X_test,Y_test))\n\n# Y_pred=dtc2.predict(X_test)\n\n# print( \" Mean absolute error is \", mean_absolute_error(Y_test,Y_pred))\n# print(\" Mean squared  error is \" , mean_squared_error(Y_test,Y_pred))\n# print(\" Median absolute error is \" ,median_absolute_error(Y_test,Y_pred)) \n# print(\"Accuracy score \" , accuracy_score(Y_test,Y_pred))\n# print(\"F1 score: \", round(f1_score(Y_test, Y_pred, average='weighted')*100,2),\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# matrix = confusion_matrix(Y_test,Y_pred, labels=[1,0])\n# print('Confusion matrix : \\n',matrix)\n\n\n# tp, fn, fp, tn = confusion_matrix(Y_test,Y_pred,labels=[1,0]).reshape(-1)\n# print('Outcome values : \\n', tp, fn, fp, tn)\n\n\n# matrix = classification_report(Y_test,Y_pred,labels=[1,0])\n# print('Classification report : \\n',matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_roc_curve(dtc2, X_test, Y_test)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In accordance with the confusion matrix, the roc curve is almost perfect.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}